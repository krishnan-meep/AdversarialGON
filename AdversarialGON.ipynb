{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From https://github.com/cwkx/GON\n",
    "#\n",
    "# requirements\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# colab requirements\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "n_channels = 3\n",
    "img_coords = 2\n",
    "\n",
    "# training info\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "num_latent = 32\n",
    "hidden_features = 256\n",
    "num_layers = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#CIFAR\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "dataloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "x, _= next(iter(dataloader))\n",
    "print(x.shape)\n",
    "\n",
    "img = np.transpose(x[0], (1, 2, 0))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the GON network (a SIREN as in https://vsitzmann.github.io/siren/)\n",
    "class SirenLayer(nn.Module):\n",
    "    def __init__(self, in_f, out_f, w0=30, is_first=False, is_last=False):\n",
    "        super().__init__()\n",
    "        self.in_f = in_f\n",
    "        self.w0 = w0\n",
    "        self.linear = nn.Linear(in_f, out_f)\n",
    "        self.is_first = is_first\n",
    "        self.is_last = is_last\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        b = 1 / self.in_f if self.is_first else np.sqrt(6 / self.in_f) / self.w0\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.uniform_(-b, b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x if self.is_last else torch.sin(self.w0 * x)\n",
    "\n",
    "def gon_model(dimensions):\n",
    "    first_layer = SirenLayer(dimensions[0], dimensions[1], is_first=True)\n",
    "    other_layers = []\n",
    "    for dim0, dim1 in zip(dimensions[1:-2], dimensions[2:-1]):\n",
    "        other_layers.append(SirenLayer(dim0, dim1))\n",
    "    final_layer = SirenLayer(dimensions[-2], dimensions[-1], is_last=True)\n",
    "    return nn.Sequential(first_layer, *other_layers, final_layer)\n",
    "\n",
    "class Res_Block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super(Res_Block, self).__init__()\n",
    "    self.C1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, 3, 1, 1))\n",
    "    self.C2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, 4, 2, 1))\n",
    "    self.CS = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, 4, 2, 1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_i = self.CS(x)\n",
    "    x = nn.functional.leaky_relu(self.C1(x))\n",
    "    x = nn.functional.leaky_relu(x_i + self.C2(x))\n",
    "    return x\n",
    "\n",
    "class Disc(nn.Module):\n",
    "  def __init__(self, img_size):\n",
    "    super(Disc, self).__init__()\n",
    "    self.h, self.w = img_size//8, img_size//8\n",
    "    self.C1 = nn.utils.spectral_norm(nn.Conv2d(3, 32, 4, 2, 1))\n",
    "    self.C2 = nn.utils.spectral_norm(nn.Conv2d(32, 64, 4, 2, 1))\n",
    "    self.C3 = nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, 2, 1))\n",
    "    self.C4 = nn.utils.spectral_norm(nn.Conv2d(128, 128, 3, 1, 1))\n",
    "\n",
    "    self.R1 = Res_Block(3, 32)\n",
    "    self.R2 = Res_Block(32, 64)\n",
    "    self.R3 = Res_Block(64, 128)\n",
    "    self.D1 = nn.utils.spectral_norm(nn.Linear(128*self.h*self.w, 1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    #x = nn.functional.leaky_relu(self.C1(x))\n",
    "    #x = nn.functional.leaky_relu(self.C2(x))\n",
    "    #x = nn.functional.leaky_relu(self.C3(x))\n",
    "    x = self.R3(self.R2(self.R1(x)))\n",
    "    x = nn.functional.leaky_relu(self.C4(x))\n",
    "    x = x.reshape(x.size(0), 128*self.h*self.w)\n",
    "    return self.D1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_mgrid(sidelen, dim=2):\n",
    "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
    "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
    "    mgrid = mgrid.reshape(-1, dim)\n",
    "    return mgrid\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "def slerp(a, b, t):\n",
    "    omega = torch.acos((a/torch.norm(a, dim=1, keepdim=True)*b/torch.norm(b, dim=1, keepdim=True)).sum(1)).unsqueeze(1)\n",
    "    res = (torch.sin((1.0-t)*omega)/torch.sin(omega))*a + (torch.sin(t*omega)/torch.sin(omega)) * b\n",
    "    return res\n",
    "\n",
    "def slerp_batch(model, z, coords):\n",
    "    lz = z.data.clone().squeeze(1)\n",
    "    col_size = int(np.sqrt(z.size(0)))\n",
    "    src_z = lz.data[:col_size].repeat(col_size,1)\n",
    "    z1, z2 = lz.data.split(lz.shape[0]//2)\n",
    "    tgt_z = torch.cat([z2, z1])\n",
    "    tgt_z = tgt_z[:col_size].repeat(col_size,1)\n",
    "    t = torch.linspace(0,1,col_size).unsqueeze(1).repeat(1,col_size).contiguous().view(batch_size,1).contiguous().to(device)\n",
    "    z_slerp = slerp(src_z, tgt_z, t)\n",
    "    z_slerp_rep = z_slerp.unsqueeze(1).repeat(1,coords.size(1),1) \n",
    "    g_slerp = model(torch.cat((coords, z_slerp_rep), dim=-1))\n",
    "    return g_slerp\n",
    "\n",
    "def gon_sample(model, recent_zs, coords):\n",
    "    zs = torch.cat(recent_zs, dim=0).squeeze(1).cpu().numpy()\n",
    "    mean = np.mean(zs, axis=0)\n",
    "    cov = np.cov(zs.T)\n",
    "    sample = np.random.multivariate_normal(mean, cov, size=batch_size)\n",
    "    sample = torch.tensor(sample).unsqueeze(1).repeat(1,coords.size(1),1).to(device).float()\n",
    "    model_input = torch.cat((coords, sample), dim=-1)\n",
    "    return model(model_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define GON architecture, for example gon_shape = [34, 256, 256, 256, 256, 3]\n",
    "#gon_shape = [img_coords+num_latent] + [hidden_features]*num_layers + [n_channels]\n",
    "gon_shape = [img_coords+num_latent] + [256, 256, 256, 256] + [n_channels]\n",
    "F = gon_model(gon_shape).to(device)\n",
    "D = Disc(img_size).to(device)\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.Adam(lr=0.0001, params=F.parameters())\n",
    "optimizerD = torch.optim.Adam(lr = 0.0005, params = D.parameters(), betas = (0.0, 0.99))\n",
    "c = torch.stack([get_mgrid(img_size, 2) for _ in range(batch_size)]).to(device) # coordinates\n",
    "print(c.shape)\n",
    "recent_zs = []\n",
    "print(f'> Number of G parameters {len(torch.nn.utils.parameters_to_vector(F.parameters()))}')\n",
    "print(f'> Number of D parameters {len(torch.nn.utils.parameters_to_vector(D.parameters()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = iter(cycle(dataloader))\n",
    "\n",
    "for step in range(10001):\n",
    "    # sample a batch of data\n",
    "    x, _ = next(train_iterator)\n",
    "    batch_size = x.size(0)\n",
    "    x = x.to(device)\n",
    "    x = x.permute(0, 2, 3, 1).reshape(batch_size, -1, n_channels)\n",
    "\n",
    "    real_label, fake_label = torch.ones(batch_size, 1).to(device), torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "    # compute the gradients of the inner loss with respect to zeros (gradient origin) ############################################\n",
    "    z = torch.zeros(batch_size, 1, num_latent).to(device).requires_grad_()\n",
    "\n",
    "    z_rep = z.repeat(1,c.size(1),1)\n",
    "    g = F(torch.cat((c[:batch_size], z_rep), dim=-1))\n",
    "    L_inner = ((g - x)**2).sum(1).mean() \n",
    "    z = -torch.autograd.grad(L_inner, [z], create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # now with z as our new latent points, optimise the data fitting loss #######################################################\n",
    "    z_rep = z.repeat(1, c.size(1), 1)\n",
    "    g = F(torch.cat((c[:batch_size], z_rep), dim=-1))\n",
    "\n",
    "    fake_output = D(g.view(-1, img_size, img_size, 3).permute(0, 3, 1, 2))\n",
    "    real_output = D(x.view(-1, img_size, img_size, 3).permute(0, 3, 1, 2))\n",
    "    errG = torch.mean(((real_output - torch.mean(fake_output)) - fake_label)**2) + torch.mean(((fake_output - torch.mean(real_output)) - real_label)**2)\n",
    "    errG /= 2\n",
    "\n",
    "    L_outer = 10*errG + ((g - x)**2).sum(1).mean() \n",
    "    optim.zero_grad()\n",
    "    L_outer.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # compute sampling statistics\n",
    "    recent_zs.append(z.detach())\n",
    "    recent_zs = recent_zs[-100:]\n",
    "\n",
    "    #Discomator ####################################################################################################\n",
    "    D.zero_grad()\n",
    "    real_images = x.view(-1, img_size, img_size, 3).permute(0, 3, 1, 2)\n",
    "\n",
    "    sample = gon_sample(F, recent_zs, c[:batch_size]).detach()[:batch_size]\n",
    "    sample = sample.view(-1, img_size, img_size, 3).permute(0, 3, 1, 2)\n",
    "\n",
    "    real_output = D(real_images)\n",
    "    fake_output = D(sample)\n",
    "    \n",
    "    errD_real = torch.mean(((real_output - torch.mean(fake_output)) - real_label)**2) \n",
    "    errD_fake = torch.mean(((fake_output - torch.mean(real_output)) - fake_label)**2)\n",
    "    errD = (errD_fake + errD_real)/2\n",
    "    errD.backward()\n",
    "    optimizerD.step()\n",
    "    \n",
    "    if step % 10 == 0 and step > 0:\n",
    "        print(f\"Step: {step}   Loss: {L_outer.item():.3f}\", \"(\", errD.item(), errG.item(), \")\")\n",
    "    if step % 100 == 0 and step > 0:\n",
    "        clear_output()\n",
    "        print(f\"Step: {step}   Loss: {L_outer.item():.3f}\")\n",
    "\n",
    "        # plot reconstructions, interpolations, and samples\n",
    "        recons = g.clone().detach()[:16]\n",
    "        for i, r in enumerate(recons):\n",
    "          recons[i] = (r - r.min())/(r.max() - r.min())\n",
    "\n",
    "        sample = gon_sample(F, recent_zs, c[:batch_size]).detach()[:16]\n",
    "        for i, r in enumerate(sample):\n",
    "          sample[i] = (r - r.min())/(r.max() - r.min())\n",
    "\n",
    "        recons = torchvision.utils.make_grid(recons.permute(0, 2, 1).reshape(-1, n_channels, img_size, img_size), nrow=8)\n",
    "        #slerps = torchvision.utils.make_grid(torch.clamp(slerp_batch(F, z.data, c), 0, 1).permute(0,2,1).reshape(-1, n_channels, img_size, img_size), nrow=8)\n",
    "        sample = torchvision.utils.make_grid(sample.permute(0,2,1).reshape(-1, n_channels, img_size, img_size), nrow = 8)\n",
    "\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.title('Reconstructions')\n",
    "        plt.imshow(recons.permute(1, 2, 0).cpu().data.numpy())\n",
    "\n",
    "        #plt.figure(figsize=(15,15))\n",
    "        #plt.title('Spherical Interpolations')\n",
    "        #plt.imshow(slerps.permute(1, 2, 0).cpu().data.numpy())\n",
    "\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.title('Samples')\n",
    "        plt.imshow(sample.permute(1, 2, 0).cpu().data.numpy())\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
